package com.ai.assistance.operit.voice

import android.content.Context
import android.util.Log
import com.ai.assistance.operit.ui.common.displays.MessageContentParser
import org.tensorflow.lite.Interpreter
import java.io.FileInputStream
import java.nio.channels.FileChannel
import com.ai.assistance.operit.ui.common.displays.MessageContentParser.Companion.ContentSegment
import com.ai.assistance.operit.util.TextSegmenter

/**
 * TextSummarizer - åŸºäºTensorFlow Liteçš„æ–‡æœ¬æ‘˜è¦æå–å™¨
 * ç”¨äºå°†èŠå¤©æ¶ˆæ¯è½¬æ¢ä¸ºæ›´é€‚åˆTTSæœ—è¯»çš„æ ¼å¼
 */
class TextSummarizer(private val context: Context) {
    companion object {
        private const val TAG = "TextSummarizer"
    }

    // TensorFlow Liteè§£é‡Šå™¨
    private var interpreter: Interpreter? = null

    // è¯æ±‡è¡¨å’Œæ ‡è®°å™¨
    private val tokenizer: SimpleTokenizer

    // åˆå§‹åŒ–
    init {
        loadMode()

        // åˆå§‹åŒ–åˆ†è¯å™¨
        tokenizer = SimpleTokenizer()
    }

    private fun loadMode() {
        val modelFile = "universal_sentence_encoder_lite.tflite"
        try {
            // å°è¯•ä»assetsåŠ è½½æ¨¡å‹
            val fileDescriptor = context.assets.openFd(modelFile)
            val inputStream = FileInputStream(fileDescriptor.fileDescriptor)
            val fileChannel = inputStream.channel
            val startOffset = fileDescriptor.startOffset
            val declaredLength = fileDescriptor.declaredLength
            val mappedByteBuffer = fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength)

            // åˆ›å»ºè§£é‡Šå™¨å¹¶é…ç½®
            val interpreterOptions = Interpreter.Options().apply {
                numThreads = 2 // æ ¹æ®è®¾å¤‡è°ƒæ•´
                useNNAPI = true // å¯ç”¨Neural Network APIåŠ é€Ÿ
            }
            interpreter = Interpreter(mappedByteBuffer, interpreterOptions)

            Log.d(TAG, "TF Liteæ¨¡å‹åŠ è½½æˆåŠŸ")
        } catch (e: Exception) {
            Log.e(TAG, "TF Liteæ¨¡å‹åŠ è½½å¤±è´¥: ${e.message}")
            interpreter = null
        }
    }

    /**
     * å¤„ç†èŠå¤©æ¶ˆæ¯ï¼Œæå–é€‚åˆTTSçš„å†…å®¹
     * @param originalContent åŸå§‹åŒ…å«XMLæ ‡ç­¾çš„å†…å®¹
     * @return å¤„ç†åé€‚åˆTTSæœ—è¯»çš„å†…å®¹
     */
    fun process(originalContent: String): String {
        // 1. è§£æXMLæ ‡ç­¾
        val contentSegments = MessageContentParser.parseContent(originalContent, true)

        // ç‰¹æ®Šå¤„ç†ï¼šæå‰æ£€æŸ¥å€’æ•°ç¬¬äºŒä¸ªå…ƒç´ æ˜¯å¦ä¸ºTextç±»å‹
        val secondLastIndex = contentSegments.size - 2
        val hasImportantSummary = secondLastIndex >= 0 &&
            contentSegments[secondLastIndex] is ContentSegment.Text

        if (!hasImportantSummary) {
            Log.e(TAG, "æ— æ³•æ‰¾åˆ°æœ‰æ•ˆçš„æ‘˜è¦")
            return ""
        }

        val content = (contentSegments[secondLastIndex] as ContentSegment.Text).content

        // æ¸…ç†Markdownæ ‡è®°å¹¶å¤„ç†emoji
        val cleanedText = cleanupMarkdownForTTS(content)

        // TODO
        val summarizeText =
            summarizeText(cleanedText)
        return cleanedText
    }

    /**
     * æ¸…ç†Markdownæ ¼å¼å¹¶ä¼˜åŒ–TTSæœ—è¯»ä½“éªŒ
     * å°†Markdownæ ¼å¼çš„æ–‡æœ¬è½¬æ¢ä¸ºé€‚åˆTTSæœ—è¯»çš„çº¯æ–‡æœ¬ï¼Œä¿ç•™å†…å®¹ä½†ç§»é™¤æˆ–è½¬æ¢æ ¼å¼æ ‡è®°
     */
    private fun cleanupMarkdownForTTS(text: String): String {
        if (text.isBlank()) return ""

        // ä¸€æ­¥æ­¥å¤„ç†å„ç§æ ¼å¼
        var result = text

        // 1. å¤„ç†ä»£ç å— - åœ¨æœ—è¯»å‰åæ·»åŠ æç¤º
        result = result.replace(Regex("```(?:[a-zA-Z]+)?\n(.*?)\n```", RegexOption.DOT_MATCHES_ALL), "ä»¥ä¸‹æ˜¯ä»£ç å—ï¼š$1 ä»£ç å—ç»“æŸã€‚")
        
        // 2. å¤„ç†è¡Œå†…ä»£ç 
        result = result.replace(Regex("`([^`]+)`"), "ä»£ç  $1")

        // 3. å¤„ç†æ ‡é¢˜ (h1-h6) - æ·»åŠ "æ ‡é¢˜ï¼š"å‰ç¼€ä»¥ä¾¿äºå¬ä¼—ç†è§£å±‚çº§
        result = result.replace(Regex("^\\s*#{6}\\s+(.+)$", RegexOption.MULTILINE), "å°æ ‡é¢˜ï¼š$1")
        result = result.replace(Regex("^\\s*#{5}\\s+(.+)$", RegexOption.MULTILINE), "å°æ ‡é¢˜ï¼š$1")
        result = result.replace(Regex("^\\s*#{4}\\s+(.+)$", RegexOption.MULTILINE), "å°æ ‡é¢˜ï¼š$1")
        result = result.replace(Regex("^\\s*#{3}\\s+(.+)$", RegexOption.MULTILINE), "å­æ ‡é¢˜ï¼š$1")
        result = result.replace(Regex("^\\s*#{2}\\s+(.+)$", RegexOption.MULTILINE), "æ¬¡æ ‡é¢˜ï¼š$1")
        result = result.replace(Regex("^\\s*#\\s+(.+)$", RegexOption.MULTILINE), "ä¸»æ ‡é¢˜ï¼š$1")

        // 4. å¤„ç†å¼•ç”¨å—
        result = result.replace(Regex("^\\s*>\\s*(.+)$", RegexOption.MULTILINE), "å¼•ç”¨ï¼š$1")
        
        // 5. å¤„ç†åŠ ç²—ã€æ–œä½“å’Œç»„åˆæ ¼å¼
        result = result.replace(Regex("\\*\\*\\*([^*]+)\\*\\*\\*"), "$1") // ç²—æ–œä½“
        result = result.replace(Regex("___([^_]+)___"), "$1") // ç²—æ–œä½“(ä¸‹åˆ’çº¿ç‰ˆ)
        result = result.replace(Regex("\\*\\*([^*]+)\\*\\*"), "$1") // åŠ ç²—
        result = result.replace(Regex("__([^_]+)__"), "$1") // åŠ ç²—(ä¸‹åˆ’çº¿ç‰ˆ)
        result = result.replace(Regex("\\*([^*]+)\\*"), "$1") // æ–œä½“
        result = result.replace(Regex("_([^_]+)_"), "$1") // æ–œä½“(ä¸‹åˆ’çº¿ç‰ˆ)
        result = result.replace(Regex("~~([^~]+)~~"), "$1") // åˆ é™¤çº¿

        // 6. å¤„ç†é“¾æ¥ - æå–é“¾æ¥æ–‡æœ¬ï¼Œçœç•¥URL
        result = result.replace(Regex("!\\[(.*?)\\]\\(.*?\\)"), "å›¾ç‰‡ï¼š$1") // å›¾ç‰‡
        result = result.replace(Regex("\\[(.*?)\\]\\(.*?\\)"), "$1") // é“¾æ¥
        result = result.replace(Regex("<(https?://[^>]+)>"), "é“¾æ¥") // ç›´æ¥URL

        // 7. å¤„ç†åˆ—è¡¨
        // ç»Ÿä¸€å¤„ç†æœ‰åºåˆ—è¡¨å’Œæ— åºåˆ—è¡¨
        result = result.replace(Regex("^\\s*[*+-]\\s+(.+)$", RegexOption.MULTILINE), "â€¢ $1")
        result = result.replace(Regex("^\\s*(\\d+)\\.\\s+(.+)$", RegexOption.MULTILINE), "$1. $2")

        // 8. å¤„ç†ä»»åŠ¡åˆ—è¡¨
        result = result.replace(Regex("^\\s*- \\[ \\]\\s+(.+)$", RegexOption.MULTILINE), "å¾…åŠäº‹é¡¹ï¼š$1")
        result = result.replace(Regex("^\\s*- \\[x\\]\\s+(.+)$", RegexOption.MULTILINE), "å·²å®Œæˆäº‹é¡¹ï¼š$1")

        // 9. å¤„ç†è¡¨æ ¼ - ç®€åŒ–è¡¨æ ¼ï¼Œä»…ä¿ç•™å†…å®¹
        // ç§»é™¤è¡¨æ ¼åˆ†éš”è¡Œ
        result = result.replace(Regex("^\\|\\s*[-:]+[-\\s|:]*[-:]+\\s*\\|$", RegexOption.MULTILINE), "")
        // å¤„ç†è¡¨æ ¼å†…å®¹è¡Œï¼Œå°†ç«–çº¿æ›¿æ¢ä¸ºé€—å·æˆ–å¥å·
        result = result.replace(Regex("^\\|(.+)\\|$", RegexOption.MULTILINE)) { matchResult ->
            val cells = matchResult.groupValues[1].split("|")
            cells.joinToString("ï¼Œ") { it.trim() } + "ã€‚"
        }

        // 10. å¤„ç†æ°´å¹³çº¿
        result = result.replace(Regex("^\\s*[-*_]{3,}\\s*$", RegexOption.MULTILINE), "åˆ†éš”çº¿ã€‚")

        // 11. å¤„ç†å¸¸è§ emoji - æ›¿æ¢ä¸ºæ–‡å­—æè¿°
        val emojiMap = mapOf(
            "â˜€ï¸" to "æ™´å¤©",
            "â›…" to "å¤šäº‘",
            "ğŸŒ§ï¸" to "é›¨å¤©",
            "â„ï¸" to "é›ªèŠ±",
            "ğŸŒ¡ï¸" to "æ¸©åº¦è®¡",
            "ğŸ’¨" to "é£",
            "ğŸ’§" to "æ°´æ»´",
            "âš ï¸" to "è­¦å‘Š",
            "ğŸŒªï¸" to "é¾™å·é£",
            "ğŸ˜€" to "ç¬‘è„¸",
            "ğŸ˜ƒ" to "å¼€å¿ƒ",
            "ğŸ˜„" to "å¤§ç¬‘",
            "ğŸ˜" to "éœ²é½¿ç¬‘",
            "ğŸ˜†" to "çœ¯çœ¼ç¬‘",
            "ğŸ˜…" to "è‹¦ç¬‘",
            "ğŸ¤£" to "ç¬‘å€’",
            "ğŸ˜‚" to "ç¬‘å“­",
            "ğŸ™‚" to "å¾®ç¬‘",
            "ğŸ™ƒ" to "å€’è„¸ç¬‘",
            "ğŸ˜‰" to "çœ¨çœ¼",
            "ğŸ˜Š" to "å«ç¬‘",
            "ğŸ˜‡" to "å¤©ä½¿ç¬‘",
            "ğŸ‘" to "èµ",
            "ğŸ‘" to "è¸©",
            "â¤ï¸" to "çˆ±å¿ƒ",
            "ğŸ’”" to "å¿ƒç¢",
            "âœ…" to "å‹¾é€‰",
            "âŒ" to "é”™è¯¯",
            "ğŸ”´" to "çº¢è‰²",
            "ğŸŸ " to "æ©™è‰²",
            "ğŸŸ¡" to "é»„è‰²",
            "ğŸŸ¢" to "ç»¿è‰²",
            "ğŸ”µ" to "è“è‰²",
            "ğŸŸ£" to "ç´«è‰²",
            "âš«" to "é»‘è‰²",
            "âšª" to "ç™½è‰²"
        )

        // æ›¿æ¢emoji
        for ((emoji, description) in emojiMap) {
            result = result.replace(emoji, description)
        }

        // 12. å¤„ç†æ•°å­¦å…¬å¼
        result = result.replace(Regex("\\$\\$(.*?)\\$\\$", RegexOption.DOT_MATCHES_ALL), "æ•°å­¦å…¬å¼ã€‚")
        result = result.replace(Regex("\\$(.*?)\\$"), "æ•°å­¦å…¬å¼ã€‚")

        // 13. å¤„ç†HTMLæ ‡ç­¾
        result = result.replace(Regex("<([a-zA-Z][a-zA-Z0-9]*)\\s*[^>]*>.*?</\\1>", RegexOption.DOT_MATCHES_ALL), "")
        result = result.replace(Regex("<[^>]+>"), "")

        // 14. å¤„ç†è½¬ä¹‰å­—ç¬¦
        result = result.replace(Regex("\\\\([\\\\`*_{}\\[\\]()#+\\-.!])"), "$1")

        // 15. å¤„ç†XMLå’Œç‰¹æ®Šæ ¼å¼
        result = result.replace(Regex("&[a-zA-Z]+;|&#[0-9]+;"), " ")

        // 16. å»é™¤å¤šä½™ç©ºè¡Œå’Œç©ºæ ¼
        result = result.replace(Regex("\n{3,}"), "\n\n") // å¤šä¸ªç©ºè¡Œæ›¿æ¢ä¸ºæœ€å¤šä¸¤ä¸ª
        result = result.replace(Regex(" {2,}"), " ") // å¤šä¸ªç©ºæ ¼æ›¿æ¢ä¸ºä¸€ä¸ª

        // 17. å¤„ç†ä¸­è‹±æ–‡ä¹‹é—´çš„ç©ºæ ¼
        result = result.replace(Regex("([a-zA-Z])([\\u4e00-\\u9fa5])"), "$1 $2") // è‹±æ–‡åé¢æ˜¯ä¸­æ–‡
        result = result.replace(Regex("([\\u4e00-\\u9fa5])([a-zA-Z])"), "$1 $2") // ä¸­æ–‡åé¢æ˜¯è‹±æ–‡
        
        // 18. ä¼˜åŒ–æ ‡ç‚¹ç¬¦å·æœ—è¯»ä½“éªŒ
        result = result.replace(Regex("([,.;:!?ï¼Œã€‚ï¼›ï¼šï¼ï¼Ÿ])([a-zA-Z\\u4e00-\\u9fa5])"), "$1 $2") // æ ‡ç‚¹åæ·»åŠ ç©ºæ ¼

        return result.trim()
    }


    /**
     * ä½¿ç”¨TensorFlow Liteæ¨¡å‹å¯¹æ–‡æœ¬è¿›è¡Œæ‘˜è¦
     */
    private fun summarizeText(text: String): String {
        try {
            if (interpreter == null) return ""

            // å‡†å¤‡è¾“å…¥æ•°æ®
            val inputText = if (text.length > 512) text.substring(0, 512) else text

            // å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯å’Œç¼–ç 
            val tokens = tokenizeText(inputText)

            // åˆ›å»ºè¾“å…¥å¼ é‡
            val inputTensor = Array(1) { IntArray(tokens.size) }
            for (i in tokens.indices) {
                inputTensor[0][i] = tokens[i]
            }

            // åˆ›å»ºè¾“å‡ºå¼ é‡ï¼ˆæ ¹æ®æ‚¨çš„æ¨¡å‹è°ƒæ•´ç»´åº¦ï¼‰
            val outputShape = interpreter!!.getOutputTensor(0).shape()
            val outputTensor = Array(1) { FloatArray(outputShape[1]) }

            // è¿è¡Œæ¨ç†
            interpreter!!.run(inputTensor, outputTensor)

            // å¤„ç†è¾“å‡ºç»“æœ
            // æ³¨æ„ï¼šè¿™é‡Œçš„å®ç°å–å†³äºæ‚¨çš„æ¨¡å‹å…·ä½“è¾“å‡ºç±»å‹
            // è¿™é‡Œå‡è®¾æ¨¡å‹è¾“å‡ºæ˜¯å¥å­é‡è¦æ€§çš„è¯„åˆ†

            // ç¤ºä¾‹ï¼šä½¿ç”¨è¾“å‡ºå‘é‡å¯¹åŸå§‹å¥å­é‡æ–°æ’åº
            val sentences = splitIntoSentences(text)

            // å¦‚æœå¥å­æ•°é‡å°‘äºè¾“å‡ºç»´åº¦ï¼Œè¿›è¡Œæˆªæ–­
            val validSize = minOf(sentences.size, outputTensor[0].size)

            if (validSize == 0) return ""

            // åˆ›å»ºå¥å­-å¾—åˆ†å¯¹
            val scoredSentences = ArrayList<Pair<String, Float>>(validSize)
            for (i in 0 until validSize) {
                scoredSentences.add(Pair(sentences[i], outputTensor[0][i]))
            }

            // é€‰æ‹©å¾—åˆ†æœ€é«˜çš„å¥å­
            val topSentences = scoredSentences
                .sortedByDescending { it.second }
                .take(2)
                .map { it.first }

            return topSentences.joinToString(" ").let {
                if (it.length > 250) it.take(250) + "..." else it
            }
        } catch (e: Exception) {
            Log.e(TAG, "æ‘˜è¦å¤„ç†å¤±è´¥: ${e.message}", e)
            // å¼‚å¸¸å¤„ç†ï¼šè¿”å›æ–‡æœ¬çš„å‰200ä¸ªå­—ç¬¦ä½œä¸ºå®‰å…¨å›é€€
            return text.take(200) + "..."
        }
    }

    /**
     * å°†æ–‡æœ¬åˆ†å‰²æˆå¥å­
     */
    private fun splitIntoSentences(text: String): List<String> {
        // ä½¿ç”¨æ›´å¤æ‚çš„å¥å­åˆ†å‰²é€»è¾‘ï¼Œå¤„ç†ä¸­è‹±æ–‡æ··åˆæƒ…å†µ
        val sentencePattern = Regex("(?<=[.ã€‚!ï¼?ï¼Ÿ]\\s)|(?<=[.ã€‚!ï¼?ï¼Ÿ])")

        return sentencePattern.split(text)
            .map { it.trim() }
            .filter { it.isNotEmpty() && it.length >= 10 } // è¿‡æ»¤æ‰å¤ªçŸ­çš„å¥å­
    }


    /**
     * å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯å¤„ç†
     */
    private fun tokenizeText(text: String): IntArray {
        // ä½¿ç”¨TextSegmenterè¿›è¡Œåˆ†è¯
        val segments = TextSegmenter.segment(text)

        // å°†åˆ†è¯ç»“æœè½¬æ¢ä¸ºæ•´æ•°ID
        // æ³¨æ„ï¼šå®é™…å®ç°éœ€è¦æ ¹æ®æ‚¨çš„æ¨¡å‹è¯æ±‡è¡¨è¿›è¡Œè°ƒæ•´
        return segments.map { it.hashCode() and 0x7FFFFFFF }.toIntArray()
    }

    private inner class SimpleTokenizer {
        // ç®€å•å®ç°ï¼Œå®é™…ä½¿ç”¨æ—¶å¯ä»¥æ›¿æ¢ä¸ºæ›´å¤æ‚çš„åˆ†è¯å™¨
        fun tokenize(text: String): IntArray {
            // ä½¿ç”¨TextSegmenterè¿›è¡Œåˆ†è¯
            val segments = TextSegmenter.segment(text)
            return segments.map { it.hashCode() and 0xFFFF }.toIntArray()
        }
    }

}